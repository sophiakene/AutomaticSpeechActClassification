{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack, save_npz, csr_matrix\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v_model(data, vector_size, window, min_count, seed=42):\n",
    "    tokenized_data = [word_tokenize(doc) for doc in data]\n",
    "    model = Word2Vec(sentences=tokenized_data, vector_size=vector_size, window=window, min_count=min_count, seed=seed)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vector(doc, model):\n",
    "    word_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(nominal_data):\n",
    "    encoder = OneHotEncoder(sparse_output=True)\n",
    "    return encoder.fit_transform(nominal_data.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep(numerical_data):\n",
    "    return numerical_data.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_vectorize(data, model, vector_size):\n",
    "    tokenized_data = [word_tokenize(doc) for doc in data]\n",
    "    X_w2v = np.array([\n",
    "        np.mean([model.wv[word] for word in doc if word in model.wv], axis=0)\n",
    "        if any(word in model.wv for word in doc)\n",
    "        else np.zeros(vector_size)\n",
    "        for doc in tokenized_data\n",
    "    ])\n",
    "    return csr_matrix(X_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v_train_model(data, vector_size=20, min_count=2, epochs=50, seed=42):\n",
    "    data = data.fillna(\"\").astype(str)\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(data)]\n",
    "    model = Doc2Vec(vector_size=vector_size, min_count=min_count, epochs=epochs, seed=seed)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v_vectorize(data, model):\n",
    "    data = data.fillna(\"\").astype(str)\n",
    "    document_vectors = [model.infer_vector(word_tokenize(doc.lower())) for doc in data]\n",
    "    return csr_matrix(document_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_boolean(boolean_data):\n",
    "    return boolean_data.astype(int).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function \n",
    "\n",
    "def vectorize(data):\n",
    "    # Set random seeds globally\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "\n",
    "    # get labels\n",
    "    y = data.Label\n",
    "\n",
    "    # text data\n",
    "    X_text = data.Cleaned_Text\n",
    "\n",
    "    # 1. Fit Vectorizers and Models\n",
    "    all_text = pd.concat([X_text]) #?\n",
    "\n",
    "    # Word2Vec\n",
    "    w2v_model = train_w2v_model(X_text, vector_size=100, window=5, min_count=25, seed=42)\n",
    "\n",
    "    # Doc2Vec (fit on all text to have a consistent latent space)\n",
    "    d2v_model = d2v_train_model(all_text, vector_size=20, min_count=2, epochs=50, seed=42)\n",
    "\n",
    "    # CountVectorizer and TfidfVectorizer (fit once on all)\n",
    "    cv = CountVectorizer(ngram_range=(1,4), max_features=100)\n",
    "    cv.fit(all_text)\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=100, ngram_range=(1,4))\n",
    "    tfidf.fit(all_text)\n",
    "\n",
    "\n",
    "    # 2. Different Vectorizations\n",
    "\n",
    "    # Count Vector for turns\n",
    "    X_bow = cv.transform(X_text)\n",
    "\n",
    "    # TF-IDF Vectors for turns\n",
    "    X_tfidf = tfidf.transform(X_text)\n",
    "\n",
    "    # Word2Vec Vectors for turns\n",
    "    X_word2vec = w2v_vectorize(X_text, w2v_model, vector_size=100)\n",
    "\n",
    "    # Doc2Vec Vectors for turns\n",
    "    X_doc2vec = d2v_vectorize(X_text, d2v_model)\n",
    "\n",
    "\n",
    "    # 3. Encode other features\n",
    "\n",
    "    X_register1 = one_hot_encode(data[\"Dialogue/Monologue\"])\n",
    "    X_register2 = one_hot_encode(data[\"Register_low_level\"])\n",
    "    X_register3 = one_hot_encode(data[\"Register\"])\n",
    "    X_pos_tags = one_hot_encode(data[\"PoS_Tags\"])\n",
    "    X_pos_bigrams = one_hot_encode(data[\"PoS_Bigrams\"])\n",
    "    X_greeting = encode_boolean(data[\"Greeting\"])\n",
    "    X_first_word = one_hot_encode(data[\"First_Word\"])\n",
    "    X_sent_length = keep(data[\"Sentence_Length\"].array)\n",
    "\n",
    "    # 4. Combine Feature Vectors\n",
    "\n",
    "    feature_vector_all = hstack([\n",
    "        X_bow, \n",
    "        X_tfidf, \n",
    "        X_word2vec, \n",
    "        X_doc2vec,\n",
    "        X_register1, X_register2, X_register3,\n",
    "        X_sent_length, X_pos_tags, X_pos_bigrams,X_greeting, X_first_word\n",
    "    ]).tocsr()\n",
    "\n",
    "    # 4. Save Feature Vectors and Labels\n",
    "\n",
    "    save_npz(\"vectorized_data.npz\", feature_vector_all)\n",
    "    np.save(\"labels.npy\", y.to_numpy())\n",
    "    np.save(\"filenames.npy\", data.Filename.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"preprocessed_data.csv\", keep_default_na=False) #path/to/preprocessed_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Register', 'Dialogue/Monologue', 'Register_low_level',\n",
       "       'Region', 'Filename', 'Original_Text', 'Cleaned_Text', 'Prev_Text',\n",
       "       'Next_Text', 'Sentence_Length', 'Label', 'PoS_Tags', 'PoS_Bigrams',\n",
       "       'Greeting', 'First_Word'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectorize_and_classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
